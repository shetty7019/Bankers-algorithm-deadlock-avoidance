{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/sLLPbOlskDBzJ6IV2XgH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shetty7019/Bankers-algorithm-deadlock-avoidance/blob/main/final_depression_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmawsTSAknWQ"
      },
      "outputs": [],
      "source": [
        "import fnmatch\n",
        "\n",
        "import os\n",
        "\n",
        "import zipfile\n",
        "\n",
        "\n",
        "def extract_files(zip_file, out_dir, delete_zip=False):\n",
        "\n",
        "    # create audio directory\n",
        "    audio_dir = os.path.join(out_dir, 'audio')\n",
        "    if not os.path.exists(audio_dir):\n",
        "        os.makedirs(audio_dir)\n",
        "\n",
        "    # create transcripts directory\n",
        "    transcripts_dir = os.path.join(out_dir, 'transcripts')\n",
        "    if not os.path.exists(audio_dir):\n",
        "        os.makedirs(transcripts_dir)\n",
        "\n",
        "    zip_ref = zipfile.ZipFile(zip_file)\n",
        "    for f in zip_ref.namelist():  # iterate through files in zip file\n",
        "        if f.endswith('.wav'):\n",
        "            zip_ref.extract(f, audio_dir)\n",
        "        elif fnmatch.fnmatch(f, '*TRANSCRIPT.csv'):\n",
        "            zip_ref.extract(f, transcripts_dir)\n",
        "    zip_ref.close()\n",
        "\n",
        "    if delete_zip:\n",
        "        os.remove(zip_file)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    dir_name = '/Volumes/Seagate Backup Plus Drive/'\n",
        "\n",
        "    out_dir = '../../depression-detect/data/raw'\n",
        "\n",
        "    for file in os.listdir(dir_name):\n",
        "        if file.endswith('.zip'):\n",
        "            zip_file = os.path.join(dir_name, file)\n",
        "            extract_files(zip_file, out_dir, delete_zip=delete_zip)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyAudioAnalysis import audioBasicIO as aIO\n",
        "from pyAudioAnalysis import audioSegmentation as aS\n",
        "import scipy.io.wavfile as wavfile\n",
        "import wave\n",
        "\n",
        "\n",
        "\n",
        "def remove_silence(filename, out_dir, smoothing=1.0, weight=0.3, plot=False):\n",
        "\n",
        "    partic_id = 'P' + filename.split('/')[-1].split('_')[0]  # PXXX\n",
        "    if is_segmentable(partic_id):\n",
        "        # create participant directory for segmented wav files\n",
        "        participant_dir = os.path.join(out_dir, partic_id)\n",
        "        if not os.path.exists(participant_dir):\n",
        "            os.makedirs(participant_dir)\n",
        "\n",
        "        os.chdir(participant_dir)\n",
        "\n",
        "        [Fs, x] = aIO.read_audio_file(filename)\n",
        "        segments = aS.silence_removal(x, Fs, 0.020, 0.020,\n",
        "                                     smooth_window=smoothing,\n",
        "                                     weight=weight,\n",
        "                                     plot=plot)\n",
        "\n",
        "        for s in segments:\n",
        "            seg_name = \"{:s}_{:.2f}-{:.2f}.wav\".format(partic_id, s[0], s[1])\n",
        "            wavfile.write(seg_name, Fs, x[int(Fs * s[0]):int(Fs * s[1])])\n",
        "\n",
        "        # concatenate segmented wave files within participant directory\n",
        "        concatenate_segments(participant_dir, partic_id)\n",
        "\n",
        "\n",
        "def is_segmentable(partic_id):\n",
        "\n",
        "    troubled = set(['P300', 'P305', 'P306', 'P308', 'P315', 'P316', 'P343',\n",
        "                    'P354', 'P362', 'P375', 'P378', 'P381', 'P382', 'P385',\n",
        "                    'P387', 'P388', 'P390', 'P392', 'P393', 'P395', 'P408',\n",
        "                    'P413', 'P421', 'P438', 'P473', 'P476', 'P479', 'P490',\n",
        "                    'P492'])\n",
        "    return partic_id not in troubled\n",
        "\n",
        "\n",
        "def concatenate_segments(participant_dir, partic_id, remove_segment=True):\n",
        "\n",
        "    infiles = os.listdir(participant_dir)  # list of wav files in directory\n",
        "    outfile = '{}_no_silence.wav'.format(partic_id)\n",
        "\n",
        "    data = []\n",
        "    for infile in infiles:\n",
        "        w = wave.open(infile, 'rb')\n",
        "        data.append([w.getparams(), w.readframes(w.getnframes())])\n",
        "        w.close()\n",
        "        if remove_segment:\n",
        "            os.remove(infile)\n",
        "\n",
        "    output = wave.open(outfile, 'wb')\n",
        "    output.setparams(data[0][0])\n",
        "\n",
        "    # write each segment to output\n",
        "    for idx in range(len(data)):\n",
        "        output.writeframes(data[idx][1])\n",
        "    output.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # directory containing raw wav files\n",
        "    dir_name = '/Users/bhumikashetty/Desktop/depression-detect/src/data/raw/audio/'\n",
        "\n",
        "    # segmented wav file\n",
        "    out_dir = '/Users/bhumikashetty/Desktop/depression-detect/src/data/interim/'\n",
        "\n",
        "    # iterate through wav files in dir_name and create a segmented wav_file\n",
        "    for file in os.listdir(dir_name):\n",
        "        if file.endswith('.wav'):\n",
        "            filename = os.path.join(dir_name, file)\n",
        "            remove_silence(filename, out_dir)\n"
      ],
      "metadata": {
        "id": "DCUEZ-SKr8hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spectrograms.py\n",
        "import numpy as np\n",
        "from numpy.lib import stride_tricks\n",
        "import os\n",
        "from PIL import Image\n",
        "import scipy.io.wavfile as wav\n",
        "\n",
        "\n",
        "\n",
        "#This script creates spectrogram matrices from wav files that can be passed to the CNN.\n",
        "\n",
        "\n",
        "def stft(sig, frameSize, overlapFac=0.5, window=np.hanning):\n",
        "\n",
        "    win = window(frameSize)\n",
        "    hopSize = int(frameSize - np.floor(overlapFac * frameSize))\n",
        "    # zeros at beginning (thus center of 1st window should be for sample nr. 0)\n",
        "    samples = np.append(np.zeros(int(np.floor(frameSize/2.0))), sig)\n",
        "    # cols for windowing\n",
        "    cols = np.ceil((len(samples) - frameSize) / float(hopSize)) + 1\n",
        "    # zeros at end (thus samples can be fully covered by frames)\n",
        "    samples = np.append(samples, np.zeros(frameSize))\n",
        "\n",
        "    frames = np.lib.stride_tricks.as_strided(samples, shape=(int(cols), frameSize),\n",
        "                                             strides=(samples.strides[0]*hopSize,\n",
        "                                                      samples.strides[0])).copy()\n",
        "    frames *= win\n",
        "\n",
        "    return np.fft.rfft(frames)\n",
        "\n",
        "def logscale_spec(spec, sr=44100, factor=20.):\n",
        "\n",
        "    timebins, freqbins = np.shape(spec)\n",
        "\n",
        "    scale = np.linspace(0, 1, freqbins) ** factor\n",
        "    scale *= (freqbins-1) / max(scale)\n",
        "    scale = np.unique(np.round(scale)).astype(int)  # Convert scale to integers\n",
        "\n",
        "    # create spectrogram with new freq bins\n",
        "    newspec = np.complex128(np.zeros([timebins, len(scale)]))\n",
        "    for i in range(0, len(scale)):\n",
        "        if i == len(scale)-1:\n",
        "            newspec[:, i] = np.sum(spec[:, scale[i]:], axis=1)\n",
        "        else:\n",
        "            newspec[:, i] = np.sum(spec[:, scale[i]:scale[i+1]], axis=1)\n",
        "\n",
        "    # list center freq of bins\n",
        "    allfreqs = np.abs(np.fft.fftfreq(freqbins*2, 1./sr)[:freqbins+1])\n",
        "    freqs = []\n",
        "    for i in range(0, len(scale)):\n",
        "        if i == len(scale)-1:\n",
        "            freqs += [np.mean(allfreqs[scale[i]:])]\n",
        "        else:\n",
        "            freqs += [np.mean(allfreqs[scale[i]:scale[i+1]])]\n",
        "\n",
        "    return newspec, freqs\n",
        "\n",
        "\n",
        "def stft_matrix(wavfile):\n",
        "  samplerate, samples = wav.read(wavfile)\n",
        "  binsize=2**10\n",
        "  x = stft(samples,binsize)\n",
        "  sshow, freq = logscale_spec(x, factor=1, sr=samplerate)\n",
        "  ims = 20.*np.log10(np.abs(sshow)/10e-6)  # amplitude to decibel\n",
        "  timebins, freqbins = np.shape(ims)\n",
        "\n",
        "  ims = np.transpose(ims)\n",
        "  ims = np.flipud(ims)\n",
        "  return(ims)\n",
        "\n",
        "stft_matrix(\"/content/drive/MyDrive/Colab_Notebooks/Major_project/Dataset/interim/P301/P301_no_silence.wav\")"
      ],
      "metadata": {
        "id": "IhQ-Hg_4s6Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dev_data.py\n",
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/Major_project/csv/train_split_Depression_AVEC2017.csv\")\n",
        "\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/Major_project/csv/dev_split_Depression_AVEC2017.csv\")\n",
        "\n",
        "df_dev = pd.concat([df_train, df_test], axis=0)\n"
      ],
      "metadata": {
        "id": "Gdk7OFHysfg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spectrograms_dict.py\n",
        "#from spectrograms import stft_matrix\n",
        "import os\n",
        "#from dev_data import df_dev\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This script builds dictionaries for the depressed and non-depressed classes\n",
        "with each participant id as the key, and the associated segmented matrix\n",
        "spectrogram representation as the value.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def build_class_dictionaries(dir_name):\n",
        "    depressed_dict = dict()\n",
        "    normal_dict = dict()\n",
        "    for subdir, dirs, files in os.walk(dir_name):\n",
        "        for file in files:\n",
        "            if file.endswith('no_silence.wav'):\n",
        "                partic_id = int(file.split('_')[0][1:])\n",
        "                # print(partic_id)\n",
        "                if in_dev_split(partic_id):\n",
        "                    wav_file = os.path.join(subdir, file)\n",
        "                    # matrix representation of spectrogram\n",
        "                    mat = stft_matrix(wav_file)\n",
        "                    depressed = get_depression_label(partic_id)  # 1 if True\n",
        "                    if depressed:\n",
        "                        depressed_dict[partic_id] = mat\n",
        "                    elif not depressed:\n",
        "                        normal_dict[partic_id] = mat\n",
        "    return depressed_dict, normal_dict\n",
        "\n",
        "\n",
        "def in_dev_split(partic_id):\n",
        "    #returns true if partic_id is in df_dev\n",
        "    return partic_id in set(df_dev['Participant_ID'].values)\n",
        "\n",
        "\n",
        "def get_depression_label(partic_id):\n",
        "    #Returns participant's PHQ8 Binary label\n",
        "    return df_dev.loc[df_dev['Participant_ID'] ==\n",
        "                      partic_id]['PHQ8_Binary'].item()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    dir_name = \"/content/drive/MyDrive/Colab_Notebooks/Major_project/Dataset/interim\"\n",
        "    depressed_dict, normal_dict = build_class_dictionaries(dir_name)\n"
      ],
      "metadata": {
        "id": "to4CeqCMtM0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#random_sampling.py\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "#from spectrogram_dicts import build_class_dictionaries\n",
        "np.random.seed(15)  # for reproducibility\n",
        "\n",
        "def determine_num_crops(depressed_dict, normal_dict, crop_width=125):\n",
        "    \"\"\"\n",
        "    Finds the shortest clip in the entire dataset which, will limit the number of samples we take from\n",
        "    each clip to make sure our classes are balanced.\n",
        "\n",
        "    \"\"\"\n",
        "    merged_dict = normal_dict.copy()\n",
        "    merged_dict.update(depressed_dict)\n",
        "\n",
        "    shortest_clip = min(merged_dict.items(), key=lambda x: x[1].shape[1])\n",
        "    shortest_pixel_width = shortest_clip[1].shape[1]\n",
        "    num_samples_from_clips = shortest_pixel_width / crop_width\n",
        "    return num_samples_from_clips\n",
        "\n",
        "def build_class_sample_dict(segmented_audio_dict, n_samples, crop_width):\n",
        "    class_samples_dict = dict()\n",
        "    for partic_id, clip_mat in segmented_audio_dict.items():\n",
        "            samples = get_random_samples(clip_mat, n_samples, crop_width)\n",
        "            class_samples_dict[partic_id] = samples\n",
        "    return class_samples_dict\n",
        "\n",
        "def get_random_samples(matrix, n_samples, crop_width):\n",
        "    \"\"\"\n",
        "    Get N random samples with width of crop_width from the numpy matrix\n",
        "    representing the participant's audio spectrogram.\n",
        "    \"\"\"\n",
        "    # crop full spectrogram into segments of width = crop_width\n",
        "    clipped_mat = matrix[:, (matrix.shape[1] % crop_width):]\n",
        "    n_splits = clipped_mat.shape[1] / crop_width\n",
        "    cropped_sample_ls = np.split(clipped_mat, n_splits, axis=1)\n",
        "\n",
        "    # get random samples\n",
        "    n_samples = min(n_samples, len(cropped_sample_ls))\n",
        "    n_samples = int(n_samples)\n",
        "    samples = random.sample(cropped_sample_ls, n_samples)\n",
        "\n",
        "    return samples\n",
        "\n",
        "def create_sample_dicts(crop_width):\n",
        "\n",
        "    # build dictionaries of participants and segmented audio matrix\n",
        "    depressed_dict, normal_dict = build_class_dictionaries(\"/content/drive/MyDrive/Colab_Notebooks/Major_project/Dataset/interim\")\n",
        "    n_samples = determine_num_crops(depressed_dict, normal_dict,\n",
        "                                    crop_width=crop_width)\n",
        "    # get n_sample random samples from each depressed participant\n",
        "    depressed_samples = build_class_sample_dict(depressed_dict, n_samples,\n",
        "                                                crop_width)\n",
        "    # get n_sample random samples from each non-depressed participant\n",
        "    normal_samples = build_class_sample_dict(normal_dict, n_samples,\n",
        "                                             crop_width)\n",
        "\n",
        "    # save depressed arrays to .npz\n",
        "    for key, _ in depressed_samples.items():\n",
        "        path = \"/content/drive/MyDrive/Colab_Notebooks/Major_project/Dataset/processed/\"\n",
        "        filename = 'D{}.npz'.format(key)\n",
        "        outfile = path + filename\n",
        "        np.savez(outfile, *depressed_samples[key])\n",
        "    # save normal arrays to .npz\n",
        "    for key, _ in normal_samples.items():\n",
        "        path =  \"/content/drive/MyDrive/Colab_Notebooks/Major_project/Dataset/processed/\"\n",
        "        filename = '/N{}.npz'.format(key)\n",
        "        outfile = path + filename\n",
        "        np.savez(outfile, *normal_samples[key])\n",
        "\n",
        "def rand_samp_train_test_split(npz_file_dir):\n",
        "\n",
        "    # files in directory\n",
        "    npz_files = os.listdir(npz_file_dir)\n",
        "\n",
        "    dep_samps = [f for f in npz_files if f.startswith('D')]\n",
        "    norm_samps = [f for f in npz_files if f.startswith('N')]\n",
        "    # calculate how many samples to balance classes\n",
        "    max_samples = min(len(dep_samps), len(norm_samps))\n",
        "\n",
        "    # randomly select max participants from each class without replacement\n",
        "    dep_select_samps = np.random.choice(dep_samps, size=max_samples,\n",
        "                                        replace=False)\n",
        "    norm_select_samps = np.random.choice(norm_samps, size=max_samples,\n",
        "                                         replace=False)\n",
        "\n",
        "    test_size = 0.2\n",
        "    num_test_samples = int(len(dep_select_samps) * test_size)\n",
        "\n",
        "    train_samples = []\n",
        "    for sample in dep_select_samps[:-num_test_samples]:\n",
        "        npz_file = npz_file_dir + '/' + sample\n",
        "        with np.load(npz_file) as data:\n",
        "            for key in data.keys():\n",
        "                train_samples.append(data[key])\n",
        "    for sample in norm_select_samps[:-num_test_samples]:\n",
        "        npz_file = npz_file_dir + '/' + sample\n",
        "        with np.load(npz_file) as data:\n",
        "            for key in data.keys():\n",
        "                train_samples.append(data[key])\n",
        "    train_labels = np.concatenate((np.ones(int(len(train_samples)/2)),\n",
        "                               np.zeros(int(len(train_samples)/2))))\n",
        "\n",
        "\n",
        "    test_samples = []\n",
        "    for sample in dep_select_samps[-num_test_samples:]:\n",
        "        npz_file = npz_file_dir + '/' + sample\n",
        "        with np.load(npz_file) as data:\n",
        "            for key in data.keys():\n",
        "                test_samples.append(data[key])\n",
        "    for sample in norm_select_samps[-num_test_samples:]:\n",
        "        npz_file = npz_file_dir + '/' + sample\n",
        "        with np.load(npz_file) as data:\n",
        "            for key in data.keys():\n",
        "                test_samples.append(data[key])\n",
        "    test_labels = np.concatenate((np.ones(int(len(test_samples)/2)),\n",
        "                                  np.zeros(int(len(test_samples)/2))))\n",
        "\n",
        "    return np.array(train_samples), train_labels, np.array(test_samples), \\\n",
        "        test_labels\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    create_sample_dicts(crop_width=125)\n",
        "\n",
        "    # random sample from particpants npz files to ensure class balance\n",
        "    train_samples, train_labels, test_samples, \\\n",
        "        test_labels = rand_samp_train_test_split( \"/content/drive/MyDrive/Colab_Notebooks/Major_project/Dataset/processed/\")\n",
        "\n",
        "    # save as npz locally\n",
        "    print(\"Saving npz file locally...\")\n",
        "    np.savez( \"/content/drive/MyDrive/Colab_Notebooks/Major_project/Dataset/processed/train_samples.npz\", train_samples)\n",
        "    np.savez( \"/content/drive/MyDrive/Colab_Notebooks/Major_project/Dataset/processed/train_labels.npz\", train_labels)\n",
        "    np.savez( \"/content/drive/MyDrive/Colab_Notebooks/Major_project/Dataset/processed/test_samples.npz\", test_samples)\n",
        "    np.savez( \"/content/drive/MyDrive/Colab_Notebooks/Major_project/Dataset/processed/test_labels.npz\", test_labels)\n",
        ""
      ],
      "metadata": {
        "id": "VEtLHxjFuJbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_metrics.py\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Plots of test/train accuracy, loss, ROC curve.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def plot_accuracy(history, model_id):\n",
        "    \"\"\"\n",
        "    Plots train and test accuracy for each epoch.\n",
        "    \"\"\"\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.savefig('/content/drive/MyDrive/Colab_Notebooks/Major_project/models/images/cnn{}_accuracy.png'.format(model_id))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_loss(history, model_id):\n",
        "    \"\"\"\n",
        "    Plots train and test loss for each epoch.\n",
        "    \"\"\"\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.savefig('/content/drive/MyDrive/Colab_Notebooks/Major_project/models/images/cnn{}_loss.png'.format(model_id))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_roc_curve(y_test, y_score, model_id):\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.05])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.savefig('/content/drive/MyDrive/Colab_Notebooks/Major_project/models/images/cnn{}_roc.png'.format(model_id))\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "el5PR64WvHpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cnn.py\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from plot_metrics import plot_accuracy, plot_loss, plot_roc_curve\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "np.random.seed(15)  # for reproducibility\n",
        "\n",
        "\n",
        "\n",
        "def retrieve_from_drive(file):\n",
        "    \"\"\"\n",
        "    Load spectrogram representation of matrices from the local file system.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file):\n",
        "        raise FileNotFoundError(f\"File '{file}' does not exist.\")\n",
        "\n",
        "    X = np.load(file)\n",
        "    return X\n",
        "\n",
        "\n",
        "def preprocess(X_train, X_test):\n",
        "\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "\n",
        "    X_train = np.array([(X - X.min()) / (X.max() - X.min()) for X in X_train])\n",
        "    X_test = np.array([(X - X.min()) / (X.max() - X.min()) for X in X_test])\n",
        "    return X_train, X_test\n",
        "\n",
        "\n",
        "def prep_train_test(X_train, y_train, X_test, y_test, nb_classes):\n",
        "\n",
        "    print('Train on {} samples, validate on {}'.format(X_train.shape[0],\n",
        "                                                       X_test.shape[0]))\n",
        "\n",
        "    # normalize to dBfS\n",
        "    X_train, X_test = preprocess(X_train, X_test)\n",
        "\n",
        "    # Convert class vectors to binary class matrices\n",
        "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "    Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "    return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "\n",
        "def keras_img_prep(X_train, X_test, img_dep, img_rows, img_cols):\n",
        "\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
        "        X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
        "        input_shape = (1, img_rows, img_cols)\n",
        "    else:\n",
        "        X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "        X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "    return X_train, X_test, input_shape\n",
        "\n",
        "\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "def cnn(X_train, y_train, X_test, y_test, batch_size,\n",
        "        nb_classes, epochs, input_shape):\n",
        "    \"\"\"\n",
        "    The Convolutional Neural Net architecture for classifying the audio clips\n",
        "    as normal (0) or depressed (1).\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), padding='valid', strides=1,\n",
        "                     input_shape=input_shape, activation='relu'))\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(4, 3), strides=(1, 3)))\n",
        "\n",
        "    model.add(Conv2D(32, (1, 3), padding='valid', strides=1,\n",
        "              input_shape=input_shape, activation='relu'))\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(1, 3), strides=(1, 3)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(nb_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adadelta',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n",
        "                        verbose=1, validation_data=(X_test, y_test))\n",
        "\n",
        "    # Evaluate accuracy on test and train sets\n",
        "    score_train = model.evaluate(X_train, y_train, verbose=1)\n",
        "    print('Train accuracy:', score_train[1])\n",
        "    score_test = model.evaluate(X_test, y_test, verbose=1)\n",
        "    print('Test accuracy:', score_test[1])\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "\n",
        "def model_performance(model, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Evaluation metrics for network performance.\n",
        "    \"\"\"\n",
        "    y_test_pred_proba = model.predict(X_test)\n",
        "    y_test_pred = np.argmax(y_test_pred_proba, axis=1)\n",
        "\n",
        "    y_train_pred_proba = model.predict(X_train)\n",
        "    y_train_pred = np.argmax(y_train_pred_proba, axis=1)\n",
        "\n",
        "    # Converting y_test back to 1-D array for confusion matrix computation\n",
        "    y_test_1d = y_test[:, 1]\n",
        "\n",
        "    # Computing confusion matrix for test dataset\n",
        "    conf_matrix = standard_confusion_matrix(y_test_1d, y_test_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    return y_train_pred, y_test_pred, y_train_pred_proba, \\\n",
        "        y_test_pred_proba, conf_matrix\n",
        "\n",
        "\n",
        "def standard_confusion_matrix(y_test, y_test_pred):\n",
        "\n",
        "    [[tn, fp], [fn, tp]] = confusion_matrix(y_test, y_test_pred)\n",
        "    return np.array([[tp, fp], [fn, tn]])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_id = input(\"Enter model id: \")\n",
        "\n",
        "    print('Retrieving from drive...')\n",
        "    X_train = retrieve_from_drive(\"/content/drive/MyDrive/Colab_Notebooks/Major_project/Dataset/processed/train_samples.npz\")\n",
        "    y_train = retrieve_from_drive(\"/content/drive/MyDrive/Colab_Notebooks/Major_project/Dataset/processed/train_labels.npz\")\n",
        "    X_test = retrieve_from_drive(\"/content/drive/MyDrive/Colab_Notebooks/Major_project/Dataset/processed/test_samples.npz\")\n",
        "    y_test = retrieve_from_drive(\"/content/drive/MyDrive/Colab_Notebooks/Major_project/Dataset/processed/test_labels.npz\")\n",
        "\n",
        "    X_train, y_train, X_test, y_test = \\\n",
        "        X_train['arr_0'], y_train['arr_0'], X_test['arr_0'], y_test['arr_0']\n",
        "\n",
        "    # CNN parameters\n",
        "    batch_size = 6\n",
        "    nb_classes = 2\n",
        "    epochs = 36\n",
        "\n",
        "    # normalalize data and prep for Keras\n",
        "    print('Processing images for Keras...')\n",
        "    X_train, X_test, y_train, y_test = prep_train_test(X_train, y_train,\n",
        "                                                       X_test, y_test,\n",
        "                                                       nb_classes=nb_classes)\n",
        "\n",
        "    # 513x125x1 for spectrogram with crop size of 125 pixels\n",
        "    img_rows, img_cols, img_depth = X_train.shape[1], X_train.shape[2], 1\n",
        "\n",
        "    # reshape image input for Keras\n",
        "    # used Theano dim_ordering (th), (# chans, # images, # rows, # cols)\n",
        "    X_train, X_test, input_shape = keras_img_prep(X_train, X_test, img_depth,\n",
        "                                                  img_rows, img_cols)\n",
        "\n",
        "    # run CNN\n",
        "    print('Fitting model...')\n",
        "    model, history = cnn(X_train, y_train, X_test, y_test, batch_size,\n",
        "                         nb_classes, epochs, input_shape)\n",
        "\n",
        "    # evaluate model\n",
        "    print('Evaluating model...')\n",
        "    y_train_pred, y_test_pred, y_train_pred_proba, y_test_pred_proba, \\\n",
        "        conf_matrix = model_performance(model, X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # save model to locally\n",
        "    print('Saving model locally...')\n",
        "    model_name = '/content/drive/MyDrive/Colab_Notebooks/Major_project/models/cnn_{}.h5'.format(model_id)\n",
        "    model.save(model_name)\n",
        "\n",
        "    # custom evaluation metrics\n",
        "    print('Calculating additional test metrics...')\n",
        "    accuracy = float(conf_matrix[0][0] + conf_matrix[1][1]) / np.sum(conf_matrix)\n",
        "    precision = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[0][1])\n",
        "    recall = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[1][0])\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    print(\"Accuracy: {}\".format(accuracy))\n",
        "    print(\"Precision: {}\".format(precision))\n",
        "    print(\"Recall: {}\".format(recall))\n",
        "    print(\"F1-Score: {}\".format(f1_score))\n",
        "\n",
        "    # plot train/test loss and accuracy. saves files in working dir\n",
        "    print('Saving plots...')\n",
        "    plot_loss(history, model_id)\n",
        "    plot_accuracy(history, model_id)\n",
        "    plot_roc_curve(y_test[:, 1], y_test_pred_proba[:, 1], model_id)\n",
        "\n"
      ],
      "metadata": {
        "id": "DeRFN5KGvqY7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}